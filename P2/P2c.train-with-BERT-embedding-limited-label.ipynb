{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13bbd9c-f756-457d-ac81-53f5073efcb6",
   "metadata": {},
   "source": [
    "<sub>Developed by SeongKu Kang, August 2025 â€” Do not distribute</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4883b6c",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Task 1: Product category classification with limited labels (Fixed BERT embeddings)\n",
    "\n",
    "In this notebook, we explore methods to improve classification performance in **limited label scenarios**.  \n",
    "Unlike the full-label setting, where all training products have annotated category labels, here only a small subset is labeled.  \n",
    "This mimics realistic situations in industry where annotations are expensive and only partially available.\n",
    "\n",
    "We will attempt two approaches to handle this challenge:\n",
    "\n",
    "1. **Label Embeddings**  \n",
    "   - Instead of treating category labels as discrete IDs, we represent each label with its **semantic embedding** (i.e., using BERT embeddings of category names).  \n",
    "   - This allows the model to leverage semantic similarity between categories (e.g., *\"knitting hooks\"* and *\"crochet hooks\"* are closer than *\"knitting hooks\"* and *\"laptops\"*).  \n",
    "   - Goal: improve generalization, especially when labeled data is sparse.  \n",
    "\n",
    "2. **Self-Training (Pseudo-Labeling)**  \n",
    "   - We first train a model on the limited labeled dataset.  \n",
    "   - Then, we use this model to generate **pseudo-labels** for the unlabeled products.  \n",
    "   - These pseudo-labeled products are added back into the training set, and the model is retrained.  \n",
    "   - Goal: effectively utilize the large pool of unlabeled data to boost classification accuracy.\n",
    "\n",
    "We aim to understand how semantic knowledge and unlabeled data can help in **limited supervision environments**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fd1a6-77ce-4a17-9a90-81764a85fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import * \n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5937f-50c1-43de-b833-204caf5a4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths\n",
    "ROOT = Path(\"dataset\") # Root dataset directory\n",
    "CORPUS_PATH = ROOT / \"corpus.jsonl\" # Product corpus file (JSON Lines): Each line contains a product ID and its associated text description.\n",
    "EMB_PATH = ROOT / \"corpus_bert_mean.pt\"\n",
    "\n",
    "# Task 1: Product category classification\n",
    "LABEL_MAP_PATH = ROOT / \"category_classification\" \n",
    "LABEL2ID_PATH = LABEL_MAP_PATH / \"label2labelid.json\" \n",
    "ID2LABEL_PATH = LABEL_MAP_PATH / \"labelid2label.json\" \n",
    "PID2LABEL_TRAIN_PATH = LABEL_MAP_PATH / \"pid2labelids_train.json\"\n",
    "PID2LABEL_TEST_PATH = LABEL_MAP_PATH / \"pid2labelids_test.json\" \n",
    "LABEL_EMB_PATH = LABEL_MAP_PATH / \"category_labels_bert_mean.pt\"\n",
    "\n",
    "# === Load limited label training set (subset of pid2labelids_train) ===\n",
    "PID2LABEL_TRAIN_LIMITED_PATH = LABEL_MAP_PATH / \"pid2labelids_train_limited.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c819562-37b3-4cf1-8969-9d559997458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2text = load_corpus(CORPUS_PATH) # load corpus\n",
    "\n",
    "label2id = load_json(LABEL2ID_PATH)\n",
    "id2label = load_json(ID2LABEL_PATH)\n",
    "pid2label_train_limited = load_json(PID2LABEL_TRAIN_LIMITED_PATH)\n",
    "pid2label_test = load_json(PID2LABEL_TEST_PATH)\n",
    "\n",
    "# loading pre-trained embeddings\n",
    "corpus_data = torch.load(EMB_PATH)  # {\"ids\": [...], \"embeddings\": Tensor}\n",
    "pid_list = corpus_data[\"ids\"]\n",
    "pid2idx = {pid: i for i, pid in enumerate(pid_list)}\n",
    "embeddings = corpus_data[\"embeddings\"]\n",
    "\n",
    "label_data = torch.load(LABEL_EMB_PATH)\n",
    "label_emb = label_data[\"embeddings\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63a132-7d39-4aee-98a0-76b7ecef8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCategoryEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pid2label, pid2idx, embeddings):\n",
    "        self.pids = list(pid2label.keys())\n",
    "        self.labels = [pid2label[pid] for pid in self.pids]\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return {\"X\": emb, \"y\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db847562-49ec-4364-b158-5b6213c72b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110700f8-692e-4ad5-8a7e-a9385f25bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test dataset and dataloader from precomputed embeddings\n",
    "test_dataset = ProductCategoryEmbeddingDataset(pid2label_test, pid2idx, embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Model dimensions\n",
    "input_dim = embeddings.shape[1]   # Size of embedding vector (feature dimension)\n",
    "num_classes = len(label2id)       # Number of category classes\n",
    "\n",
    "# Keep track of product IDs\n",
    "all_pids = set(pid_list)\n",
    "test_pids = set(pid2label_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc92bf4-f248-4619-88f6-581fef53afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset with limited labeled products\n",
    "train_dataset = ProductCategoryEmbeddingDataset(pid2label_train_limited, pid2idx, embeddings)\n",
    "\n",
    "# Split into train/validation sets (80% / 20%)\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(train_dataset) * val_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "train_split, val_split = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_split, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_split, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a2554-86c3-4136-9ca2-d08493e7735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find products without labels\n",
    "used_pids = set(pid2label_train_limited.keys())\n",
    "unlabeled_pids = all_pids - used_pids\n",
    "\n",
    "# Ratio of unlabeled products in the whole corpus\n",
    "unlabeled_ratio = len(unlabeled_pids) / len(all_pids)\n",
    "\n",
    "print(f\"[Unlabeled] {len(unlabeled_pids)} / {len(all_pids)} = {unlabeled_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753aaa58-b2fd-4368-839d-68a52a2fc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'valid':{}, 'test': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1100f-b083-488b-916c-f87bc697bafc",
   "metadata": {},
   "source": [
    "## [Part A] Approach 1: Label Embedding Classifier\n",
    "\n",
    "In the standard classification setup, each label is represented as a one-hot ID, and the model learns a linear classifier over these IDs.  \n",
    "This ignores the fact that **labels themselves carry semantic meaning** (e.g., *\"Knitting & Crochet\"* is semantically closer to *\"Sewing\"* than to *\"Laptops\"*).  \n",
    "\n",
    "To exploit this, we use **label embeddings**:  \n",
    "- Each category is represented by a dense embedding vector (e.g., obtained by encoding the label text with BERT).  \n",
    "- Instead of learning a weight vector for each class independently, the model projects the input product embedding into the same space as the label embeddings.  \n",
    "- Prediction is made by computing the **inner product (similarity)** between the projected input and all label embeddings.  \n",
    "\n",
    "Formally:  \n",
    "\n",
    "$$\n",
    "\\text{logits}(x) = \\text{Proj}(x) \\cdot E^T\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $\\text{Proj}(x)$ is the projected input representation,  \n",
    "- $E$ is the matrix of label embeddings.  \n",
    "\n",
    "**Benefits in limited supervision:**  \n",
    "1. **Semantic transfer** â€“ labels that are semantically related share embedding space, so the model can generalize better to underrepresented classes.  \n",
    "2. **Compactness** â€“ the model only needs to learn a projection into the label embedding space, rather than learning an independent classifier for each label.  \n",
    "\n",
    "We can also choose to keep the label embeddings **fixed** (frozen, relying purely on semantic prior) or **trainable** (allowing fine-tuning for the specific dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7f1f3-c1d3-4714-827c-274149c3cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Classifier that uses label embeddings to make predictions\n",
    "class InnerProductClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, label_embeddings, trainable_label_emb=True):\n",
    "        super().__init__()\n",
    "        # Project input features into the same dimension as label embeddings\n",
    "        self.proj = nn.Linear(input_dim, label_embeddings.size(1))\n",
    "\n",
    "        if trainable_label_emb:\n",
    "            # Label embeddings are trainable parameters\n",
    "            self.label_emb = nn.Parameter(label_embeddings.clone())\n",
    "        else:\n",
    "            # Label embeddings are fixed (not updated during training)\n",
    "            self.register_buffer(\"label_emb\", label_embeddings.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input feature vectors\n",
    "        x_proj = self.proj(x)\n",
    "        # Compute logits as similarity with each label embedding\n",
    "        logits = torch.matmul(x_proj, self.label_emb.T)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c01e30-35e9-41ca-9370-68b8d4222d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InnerProductClassifier(input_dim, label_emb).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f506a10-7d4c-4403-9676-44bca80eea54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_acc = -1\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    val_result = evaluate(model, val_loader, device=device)\n",
    "    val_acc = val_result[\"accuracy\"]\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    is_improved = val_acc > best_val_acc\n",
    "    print_eval_result(val_result, stage=\"val\", is_improved=is_improved)\n",
    "\n",
    "    # === Test ===\n",
    "    test_result = evaluate(model, test_loader, device=device)\n",
    "    test_acc = test_result[\"accuracy\"]\n",
    "    test_acc_list.append(test_acc)\n",
    "    print_eval_result(test_result, stage=\"test\")\n",
    "\n",
    "    # === Update best model ===\n",
    "    if is_improved:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"[Early Stopping] No improvement for {patience} consecutive epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba6647-f63a-4c7f-95d0-375f430c684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "final_test_result = evaluate(model, test_loader, device=device)\n",
    "print_eval_result(final_test_result, stage=\"final_test\")\n",
    "\n",
    "results_dict['valid']['label_train'] = val_acc_list[:]\n",
    "results_dict['test']['label_train'] = test_acc_list[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74249074-e22d-4e57-a656-1452fa8a49e9",
   "metadata": {},
   "source": [
    "---\n",
    "## [Part B] Approach 2. Self-training with Pseudo-labeling\n",
    "\n",
    "In this experiment, we extend supervised training with **self-training**:  \n",
    "- The model first learns on the **labeled training set**.  \n",
    "- Then, it generates predictions for **unlabeled data**.  \n",
    "- If the model is confident enough (probability above a threshold, say `0.9`), those predictions are added as **pseudo-labels** and used for further training.  \n",
    "\n",
    "### ðŸ“ Your Task: Assignment Instructions\n",
    "\n",
    "Now, your task is to implement the training loop with self-training:\n",
    "\n",
    "#### Key Components\n",
    "1. **Base training loop**  \n",
    "   - At each epoch, the model trains on the current dataset.  \n",
    "   - If pseudo-labeled data exists, the training dataset is extended with them.  \n",
    "\n",
    "2. **Validation & Test evaluation**  \n",
    "   - After training, the model is evaluated on validation and test sets.  \n",
    "   - Best validation accuracy is tracked for **early stopping**.  \n",
    "\n",
    "3. **Pseudo-label generation (every x epochs)**  \n",
    "   - The model predicts on the unlabeled set.  \n",
    "   - Predictions with confidence â‰¥ `threshold` are kept as pseudo-labels.  \n",
    "   - These are added to the training dataset for subsequent epochs.  \n",
    "\n",
    "4. **Early stopping**  \n",
    "   - Training stops if there is **no validation improvement** for `patience` epochs.  \n",
    "\n",
    "#### Why this helps\n",
    "- With limited labeled data, the model may underfit.  \n",
    "- Pseudo-labeling allows us to **leverage the large unlabeled set**, gradually improving the classifier.  \n",
    "- Using a confidence threshold avoids propagating too much noise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e3621-2054-4273-8df0-8896727debd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Unlabeled dataset: provides product embeddings without labels (for pseudo-label prediction)\n",
    "class UnlabeledEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pids, pid2idx, embeddings):\n",
    "        self.pids = list(pids)                       # list of product IDs\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]  # map PIDs to embedding indices\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"pid\": self.pids[idx], \"X\": self.embeddings[self.indices[idx]]}\n",
    "\n",
    "# Pseudo-labeled dataset: stores embeddings with assigned pseudo-labels for training\n",
    "class TensorDatasetFromVectors(Dataset):\n",
    "    def __init__(self, X_list, y_list):\n",
    "        self.X = torch.stack(X_list)                      # list of embeddings -> tensor\n",
    "        self.y = torch.tensor(y_list, dtype=torch.long)   # pseudo-labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"X\": self.X[idx], \"y\": self.y[idx]}       # embedding + pseudo-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee868d-af60-44e5-8555-ae337c780a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlabeled dataset loader: provide embeddings of unlabeled products for pseudo-labeling\n",
    "unlabeled_dataset = UnlabeledEmbeddingDataset(unlabeled_pids, pid2idx, embeddings)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35443b6a-12af-416a-b6cb-343fcc48efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InnerProductClassifier(input_dim, label_emb).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f62eb-bc28-4b30-821b-6eda78b98800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "## === Implement training loop with self-training ===\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d1610-211b-443d-8729-aac90fe2f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Evaluation ===\n",
    "print(\"\\n[Final Evaluation on Best Model]\")\n",
    "model.load_state_dict(best_model_state)\n",
    "final_test_result = evaluate(model, test_loader, device=device)\n",
    "print_eval_result(final_test_result, stage=\"final test\", is_improved=True)\n",
    "\n",
    "results_dict['valid']['label_self'] = val_acc_list[:]\n",
    "results_dict['test']['label_self'] = test_acc_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1441-ee52-4e38-b398-748351af1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_dict, split=\"valid\")\n",
    "plot_results(results_dict, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c6f2f-3aed-4f67-87f5-1c477ac970bf",
   "metadata": {},
   "source": [
    "## Prepare Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8e3d5-a984-4989-9cf4-83f34d5b951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1. Load test IDs ===\n",
    "ROOT = Path(\"dataset\") # Root dataset directory\n",
    "LABEL_MAP_PATH = ROOT / \"category_classification\"\n",
    "TEST_IDS_PATH = LABEL_MAP_PATH / \"task1_test_ids.csv\"\n",
    "\n",
    "test_ids_df = pd.read_csv(TEST_IDS_PATH)  # has column \"id\"\n",
    "test_ids = test_ids_df[\"id\"].tolist()\n",
    "\n",
    "# === 2. Custom Dataset (no labels) ===\n",
    "class ProductCategoryTestDataset(Dataset):\n",
    "    def __init__(self, pids, pid2idx, embeddings):\n",
    "        self.pids = pids\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "        self.vecs = embeddings \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        emb = self.vecs[self.indices[idx]]\n",
    "        return {\"X\": torch.tensor(emb, dtype=torch.float)}\n",
    "\n",
    "# === 3. Build dataset and loader ===\n",
    "test_dataset_kaggle = ProductCategoryTestDataset(test_ids, pid2idx, embeddings)\n",
    "test_loader_kaggle = DataLoader(test_dataset_kaggle, batch_size=64)\n",
    "\n",
    "# === 4. Run predictions ===\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_kaggle:\n",
    "        X = batch[\"X\"].to(device)  \n",
    "        logits = model(X)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "# === 5. Build submission file ===\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"label\": all_preds\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = ROOT / \"submission/P2_submission.csv\"\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"Submission file saved to {SUBMISSION_PATH}\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esci-env",
   "language": "python",
   "name": "esci-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
