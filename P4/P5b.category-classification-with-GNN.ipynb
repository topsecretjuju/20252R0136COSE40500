{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b80ec5-1177-4c76-8362-64d540aca03c",
   "metadata": {},
   "source": [
    "<sub>Developed by SeongKu Kang, August 2025 â€” Do not distribute</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce80c7-ed15-47f1-b857-8e48ec0ab4f3",
   "metadata": {},
   "source": [
    "# Product Category Classification with Label GNN\n",
    "\n",
    "This notebook builds on the previous assignment **P2c: train-with-BERT-embedding-limited-label**.  \n",
    "The setup is similar, but here we take one step further by **leveraging label structure information**.\n",
    "\n",
    "---\n",
    "\n",
    "### Recall: Label Embedding (Inner Product) Classifier\n",
    "- Each **data instance** (e.g., a product description) is projected into a hidden space.  \n",
    "- Each **label** has its own embedding vector.  \n",
    "- Classification is performed via an **inner product** between the projected instance and label embeddings.  \n",
    "\n",
    "Formally:\n",
    "$$\n",
    "\\text{logits}(x) = \\text{Proj}(x) \\cdot E^T\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $\\text{Proj}(x)$ is the projected input representation,  \n",
    "- $E$ is the matrix of label embeddings.  \n",
    "\n",
    "Originally, label embeddings $E$ were initialized with **BERT mean-pooled representations** of the label names.  \n",
    "One small difference from the previous P2 assignment is that we now use only leaf-level categories for the label embeddings.  \n",
    "\n",
    "---\n",
    "\n",
    "### Whatâ€™s New Here?\n",
    "- In addition to BERT initialization, we are now given **a graph of label relationships**, derived from the category hierarchy.  \n",
    "  - Example: if two categories share the same parent (e.g., *â€œiPhone Casesâ€* and *â€œiPhone Chargersâ€* under *â€œiPhone Accessoriesâ€*), we connect them with an edge.  \n",
    "  - We provide the **normalized adjacency matrix**, so your main task is to **implement the GNN part**.\n",
    "\n",
    "- To enrich the label embeddings, we propagate information over this label graph using a **Graph Neural Network (GNN)**:\n",
    "  $$\n",
    "  E' = \\text{GNN}(E, A_{\\text{label}})\n",
    "  $$\n",
    "  where $E$ are the initial label embeddings and $A_{\\text{label}}$ is the adjacency between labels.  \n",
    "\n",
    "- Final classification then uses these **graph-enriched label embeddings**:\n",
    "  $$\n",
    "  \\text{logits}(x) = \\text{Proj}(x) \\cdot (E')^T\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1ed06-a6f0-4f84-a10f-04265abebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import * \n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c0716-6372-4e5e-b51a-8d366e32a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths\n",
    "ROOT = Path(\"dataset\") # Root dataset directory\n",
    "CORPUS_PATH = ROOT / \"corpus.jsonl\" # Product corpus file (JSON Lines): Each line contains a product ID and its associated text description.\n",
    "EMB_PATH = ROOT / \"corpus_bert_mean.pt\"\n",
    "\n",
    "# Task 1: Product category classification\n",
    "LABEL_MAP_PATH = ROOT / \"category_classification\" \n",
    "LABEL2ID_PATH = LABEL_MAP_PATH / \"label2labelid.json\" \n",
    "ID2LABEL_PATH = LABEL_MAP_PATH / \"labelid2label.json\" \n",
    "PID2LABEL_TRAIN_LIMITED_PATH = LABEL_MAP_PATH / \"pid2labelids_train_limited.json\" \n",
    "PID2LABEL_TEST_PATH = LABEL_MAP_PATH / \"pid2labelids_test.json\" \n",
    "LABEL_EMB_PATH = LABEL_MAP_PATH / \"category_labels_leaf_bert_mean.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279150aa-c39a-4ce4-bc07-9a58dc15c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2text = load_corpus(CORPUS_PATH) # load corpus\n",
    "\n",
    "label2id = load_json(LABEL2ID_PATH)\n",
    "id2label = load_json(ID2LABEL_PATH)\n",
    "pid2label_train_limited = load_json(PID2LABEL_TRAIN_LIMITED_PATH)\n",
    "pid2label_test = load_json(PID2LABEL_TEST_PATH)\n",
    "\n",
    "# loading pre-trained embeddings\n",
    "corpus_data = torch.load(EMB_PATH)  # {\"ids\": [...], \"embeddings\": Tensor}\n",
    "pid_list = corpus_data[\"ids\"]\n",
    "pid2idx = {pid: i for i, pid in enumerate(pid_list)}\n",
    "embeddings = corpus_data[\"embeddings\"]\n",
    "\n",
    "label_data = torch.load(LABEL_EMB_PATH)\n",
    "label_emb = label_data[\"embeddings\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e98723-2058-45f4-898c-418fecfd7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCategoryEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pid2label, pid2idx, embeddings):\n",
    "        self.pids = list(pid2label.keys())\n",
    "        self.labels = [pid2label[pid] for pid in self.pids]\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return {\"X\": emb, \"y\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464a3b2-2066-4a86-a64b-0e65ad5b398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778ac3d-1ead-4f27-a7f8-477c7dc15b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test dataset and dataloader from precomputed embeddings\n",
    "test_dataset = ProductCategoryEmbeddingDataset(pid2label_test, pid2idx, embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Model dimensions\n",
    "input_dim = embeddings.shape[1]   # Size of embedding vector (feature dimension)\n",
    "num_classes = len(label2id)       # Number of category classes\n",
    "\n",
    "# Keep track of product IDs\n",
    "all_pids = set(pid_list)\n",
    "test_pids = set(pid2label_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21077e56-c6b0-40f3-83a2-ec5a28ce56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset with limited labeled products\n",
    "train_dataset = ProductCategoryEmbeddingDataset(pid2label_train_limited, pid2idx, embeddings)\n",
    "\n",
    "# Split into train/validation sets (80% / 20%)\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(train_dataset) * val_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "train_split, val_split = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_split, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_split, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8092c1-b506-40fb-93c0-640a71cf4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find products without labels\n",
    "used_pids = set(pid2label_train_limited.keys())\n",
    "unlabeled_pids = all_pids - used_pids\n",
    "\n",
    "# Ratio of unlabeled products in the whole corpus\n",
    "unlabeled_ratio = len(unlabeled_pids) / len(all_pids)\n",
    "\n",
    "print(f\"[Unlabeled] {len(unlabeled_pids)} / {len(all_pids)} = {unlabeled_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09962b-facc-4f93-b5d3-f7d9893d8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'valid':{}, 'test': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c887ae-b1da-4bb8-903b-8c14f9261b98",
   "metadata": {},
   "source": [
    "## [Part A]: revisit Classification with label embedding\n",
    "\n",
    "This part is identical to the previous assignment P2c: train-with-BERT-embedding-limited-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9600d1-c280-4237-bf5b-036655e3edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier that uses label embeddings to make predictions\n",
    "class InnerProductClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, label_embeddings, trainable_label_emb=True):\n",
    "        super().__init__()\n",
    "        # Project input features into the same dimension as label embeddings\n",
    "        self.proj = nn.Linear(input_dim, label_embeddings.size(1))\n",
    "\n",
    "        if trainable_label_emb:\n",
    "            # Label embeddings are trainable parameters\n",
    "            self.label_emb = nn.Parameter(label_embeddings.clone())\n",
    "        else:\n",
    "            # Label embeddings are fixed (not updated during training)\n",
    "            self.register_buffer(\"label_emb\", label_embeddings.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input feature vectors\n",
    "        x_proj = self.proj(x)\n",
    "        # Compute logits as similarity with each label embedding\n",
    "        logits = torch.matmul(x_proj, self.label_emb.T)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f89857-8a99-4d61-947c-a76c9a0e9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InnerProductClassifier(input_dim, label_emb).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fb6a1-95a3-4799-bac9-a19d5235ab83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_acc = -1\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    val_result = evaluate(model, val_loader, device=device)\n",
    "    val_acc = val_result[\"accuracy\"]\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    is_improved = val_acc > best_val_acc\n",
    "    print_eval_result(val_result, stage=\"val\", is_improved=is_improved)\n",
    "\n",
    "    # === Test ===\n",
    "    test_result = evaluate(model, test_loader, device=device)\n",
    "    test_acc = test_result[\"accuracy\"]\n",
    "    test_acc_list.append(test_acc)\n",
    "    print_eval_result(test_result, stage=\"test\")\n",
    "\n",
    "    # === Update best model ===\n",
    "    if is_improved:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"[Early Stopping] No improvement for {patience} consecutive epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ac7c-c697-4dcc-aef2-1ae3e8a8361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "final_test_result = evaluate(model, test_loader, device=device)\n",
    "print_eval_result(final_test_result, stage=\"final_test\")\n",
    "\n",
    "results_dict['valid']['label_train'] = val_acc_list[:]\n",
    "results_dict['test']['label_train'] = test_acc_list[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101183d2-45fb-4a5f-99ee-d3c7c3353ad3",
   "metadata": {},
   "source": [
    "## [Part B] Your Task: Implement Label GCN and GCN-Enhanced Classifier\n",
    "\n",
    "In this exercise, you will implement a **Graph Convolutional Network (GCN)** for label embeddings, and use it to build a **GCN-enhanced classifier** for product category classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Label GCN (`LabelGCN`)\n",
    "- Input:\n",
    "  - Initial label embeddings $E \\in \\mathbb{R}^{\\text{num\\_labels} \\times d}$\n",
    "  - Normalized adjacency matrix $A_{\\text{hat}} \\in \\mathbb{R}^{\\text{num\\_labels} \\times \\text{num\\_labels}}$\n",
    "- For each layer:\n",
    "  1. **Message passing:** $H \\leftarrow A_{\\text{hat}} H$\n",
    "  2. **Linear transform:** $H \\leftarrow HW$\n",
    "  3. **Activation + Dropout (optional):** Apply ReLU and Dropout, except for the last layer\n",
    "- Output: Refined label embeddings $E' \\in \\mathbb{R}^{\\text{num\\_labels} \\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. GCN-Enhanced Classifier (`GCNEnhancedClassifier`)\n",
    "- Goal: Classify a document by comparing its embedding with **graph-enriched label embeddings**.\n",
    "- Steps:\n",
    "  1. **Projection:** Project input document embeddings into the same space as label embeddings.\n",
    "  2. **Label GCN:** Refine label embeddings using the GCN.\n",
    "  3. **Inner Product Scoring:** Compute logits by inner product between projected documents and refined label embeddings:\n",
    "     $$\n",
    "     \\text{logits}(x) = \\text{Proj}(x) \\cdot (E')^T\n",
    "     $$\n",
    "- Input: \n",
    "  - $x \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{input\\_dim}}$ (document embeddings)\n",
    "  - Initial label embeddings $E$\n",
    "  - Label adjacency $A_{\\text{hat}}$\n",
    "- Output: \n",
    "  - $\\text{logits}(x) \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{num\\_labels}}$\n",
    "\n",
    "---\n",
    "You need to implement:\n",
    "1. **LabelGCN**\n",
    "   - Define learnable weights for each layer\n",
    "   - Implement message passing and transformations\n",
    "   - Add ReLU + Dropout (except the last layer)\n",
    "\n",
    "2. **GCNEnhancedClassifier**\n",
    "   - Define projection layer for documents\n",
    "   - Apply GCN to refine label embeddings\n",
    "   - Compute logits with inner product\n",
    "\n",
    "ðŸ‘‰ Start from the provided skeleton code and fill in the missing parts (`# TODO`).\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… What to submit\n",
    "- No Kaggle submission this time.  \n",
    "- Submit **only your completed notebook** via LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4f33a-00dd-4898-8c93-bfb07bb9b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed normalized adjacency matrix for label graph\n",
    "# (You don't need to worry about how it's generated â€” just use A_hat directly)\n",
    "A_hat = build_leaf_adj(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492d973-2663-4920-9528-32a61ceb64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Your Task: Implement Label GCN and GCN-Enhanced Classifier\n",
    "# ==========================================================\n",
    "\n",
    "class LabelGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Graph Convolutional Network (GCN) encoder for label embeddings.\n",
    "\n",
    "    Each layer should perform the following steps:\n",
    "        1. Aggregate neighbor embeddings: H <- A_hat @ H\n",
    "        2. Linear transformation: H <- H @ W\n",
    "        3. (Optional) Apply ReLU and Dropout (skip for the last layer)\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): Dimension of label embeddings.\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # TODO: Define learnable weight matrices (list of emb_dim x emb_dim parameters)\n",
    "        # Hint: Use nn.ParameterList and Xavier uniform initialization\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, H, A_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            H (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "            A_hat (torch.Tensor): Normalized adjacency matrix, shape (num_labels, num_labels).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated label embeddings, shape (num_labels, emb_dim).\n",
    "        \"\"\"\n",
    "        # TODO: Implement multi-layer GCN\n",
    "        # for each layer:\n",
    "        #   1) propagate messages: H = A_hat @ H\n",
    "        #   2) linear transform: H = H @ W\n",
    "        #   3) if not last layer: apply ReLU + Dropout\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GCNEnhancedClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier that combines:\n",
    "      - Document representations projected into label space\n",
    "      - Label embeddings refined by a GCN over the label graph\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of input document embeddings.\n",
    "        label_init_emb (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "        A_hat (torch.Tensor): Normalized adjacency matrix of labels, shape (num_labels, num_labels).\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, label_init_emb, A_hat, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # TODO: \n",
    "        # 1. Define projection layer (input_dim -> emb_dim)\n",
    "        # 2. Define GCN encoder for label embeddings\n",
    "        # 3. Make label_init_emb trainable (nn.Parameter)\n",
    "        # 4. Register adjacency matrix (use register_buffer)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings for documents, shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for classification, shape (batch_size, num_labels).\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # 1. Refine label embeddings using GCN\n",
    "        # 2. Project input x into label embedding space (+ dropout)\n",
    "        # 3. Compute similarity (inner product) between x_proj and label_emb\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c2e60-e419-4ceb-8f10-0c27c12304eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNEnhancedClassifier(embeddings.size(1), label_emb, A_hat.to(device), num_layers=1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59642d80-2902-4a62-b3c8-33ed8067db56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_acc = -1\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    val_result = evaluate(model, val_loader, device=device)\n",
    "    val_acc = val_result[\"accuracy\"]\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    is_improved = val_acc > best_val_acc\n",
    "    print_eval_result(val_result, stage=\"val\", is_improved=is_improved)\n",
    "\n",
    "    # === Test ===\n",
    "    test_result = evaluate(model, test_loader, device=device)\n",
    "    test_acc = test_result[\"accuracy\"]\n",
    "    test_acc_list.append(test_acc)\n",
    "    print_eval_result(test_result, stage=\"test\")\n",
    "\n",
    "    # === Update best model ===\n",
    "    if is_improved:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"[Early Stopping] No improvement for {patience} consecutive epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc511b0c-00c2-4dda-b0d4-d02dc66ad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(best_model_state)\n",
    "final_test_result = evaluate(model, test_loader, device=device)\n",
    "print_eval_result(final_test_result, stage=\"final_test\")\n",
    "\n",
    "results_dict['valid']['label_GNN_train'] = val_acc_list[:]\n",
    "results_dict['test']['label_GNN_train'] = test_acc_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0692523-f14b-4d2a-86b2-da9f57936b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_dict, split=\"valid\")\n",
    "plot_results(results_dict, split=\"valid\", metric='F1-macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esci-env",
   "language": "python",
   "name": "esci-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
